{"name":"Practical Machine Learning Course Project","tagline":"","body":"***Ben Piggot***\r\n  \r\n***September 17, 2014***\r\n\r\n### Synopsis\r\nThis brief analysis makes use of data used by Velloso et al. in their 2013 study, \"Qualitative Activity Recognition of Weight Lifting Exercises.\" (See: http://groupware.les.inf.puc-rio.br/har for more information). Using this data, it creates a model designed to predict the manner in which individuals perform weightlifting exercises.\r\n\r\n### Data Processing\r\nMy first step is to load in the training set data I use to build my model from the url below. Additionally, I load the plyr, caret, and randomForest packages.\r\n\r\n```{r, echo=TRUE}\r\nmyurl <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\r\ndownload.file(url=myurl, destfile=\"pml-training.csv\", method=\"curl\")\r\nTrainingSet <- read.csv(\"pml-training.csv\")\r\nlibrary(plyr)\r\nlibrary(caret)\r\nlibrary(randomForest)\r\n```\r\nOnce the data is loaded into my working directory, I clean the original training set. I remove variables that largely consist of missing data and variables that do not measure physical activity. I also eliminate variables that R falsely recognizes to be factor variables, but which in fact mostly contain missing or garbled information in the original .csv file. I make sure to remove and then re-attach the \"classe\" variability to my training set before eliminating the rest of the factors in the Training Set. The \"classe\" variable measures the outcome of interest in my analysis. As a result of my cleaning, the number of variables I will use in my model is reduced from 161 to 53.\r\n\r\n```{r, echo=TRUE}\r\nCleanedTrainingSet <- TrainingSet[ , colSums(is.na(TrainingSet)) < 15000] \r\nCleanedTrainingSet <- CleanedTrainingSet[,-c(1:7)]\r\nCleanedTrainingSet1 <- CleanedTrainingSet[,-86]\r\nCleanedTrainingSet1 <- CleanedTrainingSet1[, !sapply(CleanedTrainingSet1, is.factor)]\r\nCleanedTrainingSet1 <- cbind(CleanedTrainingSet1, CleanedTrainingSet$classe)\r\nCleanedTrainingSet1 <- rename(CleanedTrainingSet1, c(\"CleanedTrainingSet$classe\"=\"classe\"))\r\n```\r\nNext, I subset the cleaned training set into two halves. The first half will be used to construct my model; the second half will be used to cross-validate the model I build.\r\n\r\n```{r, echo=TRUE}\r\nset.seed(21)\r\nSample1<- sample(1:dim(CleanedTrainingSet1)[1],size=dim(CleanedTrainingSet1)[1]/2,replace=F)\r\nSampleTrain <- CleanedTrainingSet1[Sample1,]\r\nCVTrain <- CleanedTrainingSet1[-Sample1,]\r\n```\r\n### Building a Model\r\nI then build my model, predicting the classe variable utilizing a Random Forest algorithm as called inside the train() function. I then use this model to predict the values of the sample from which I constructed my model (SampleTrain). The results are then stored in a confusion matrix.\r\n\r\n```{r, echo=TRUE}\r\nRFTrain <- train(classe ~., method=\"rf\", data=SampleTrain)\r\nPrediction1 <- predict(RFTrain$finalModel, SampleTrain)\r\nPrediction1.Summary <- confusionMatrix(SampleTrain$classe, Prediction1)\r\nPrediction1.Summary\r\n```\r\n\r\n### Cross-Validating the Model\r\nThe model I have built performs exceptionally well, as it predicts the classe outcome with perfect accuracy. However, this level of accuracy could be misleading as it might be the result of overfitting. Therefore, I cross-validate my model on two samples drawn at random from the half of the training set I did not use to construct my model (CVTrain).\r\n\r\n```{r, echo=TRUE}\r\nset.seed(51)\r\nSample2 <- sample(1:dim(CVTrain)[1],size=dim(CVTrain)[1]/20,replace=F)\r\nCVSample1 <- CVTrain[Sample2,]\r\nPrediction2 <- predict(RFTrain$finalModel, CVSample1)\r\nPrediction2.Summary <- confusionMatrix(CVSample1$classe, Prediction2)\r\nPrediction2.Summary\r\n```\r\nThe model proves highly accurate in its first out-of-sample test: it correctly predicts 99.2% of the actual values. Below, the second out-of-sample prediction demonstrates similar levels of accuracy: it correctly predicts 98.8% of the actual values. Accordingly, I expect the out-of-sample error for my model to be approximately 1%.\r\n\r\n```{r, echo=TRUE}\r\nset.seed(81)\r\nSample3 <- sample(1:dim(CVTrain)[1],size=dim(CVTrain)[1]/20,replace=F)\r\nCVSample2 <- CVTrain[Sample3,]\r\nPrediction3 <- predict(RFTrain$finalModel, CVSample2)\r\nPrediction3.Summary <- confusionMatrix(CVSample2$classe, Prediction3)\r\nPrediction3.Summary\r\n```\r\n### Testing the Model\r\nNow that I am now quite confident my model accurately predicts exercise style as measured by the \"classe\" variable, I load in the testing set data. I then clean the testing set using the same procedures I used to clean the training set. \r\n\r\n```{r, echo=TRUE}\r\nmyurl <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\r\ndownload.file(url=myurl, destfile=\"pml-testing.csv\", method=\"curl\")\r\nTestingSet <- read.csv(\"pml-testing.csv\")\r\nCleanedTestingSet <- TestingSet[ , colSums(is.na(TestingSet)) < 10] \r\nCleanedTestingSet <- CleanedTestingSet[,-c(1:7)]\r\nCleanedTestingSet <- CleanedTestingSet[, !sapply(CleanedTestingSet, is.factor)]\r\n```\r\nOnce the testing set has been properly prepared, I predict its \"classe\" values using my model.\r\n\r\n```{r, echo=TRUE}\r\nPredictionFinal <- predict(RFTrain$finalModel, CleanedTestingSet)\r\nPredictionFinal\r\n```\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}